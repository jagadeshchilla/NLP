## Natural Language Processing

### 1. Tokenization
- It is a process that convert paragraphs/sentences into tokens

paragraphs ----Tokenize-----> Sentences ----Tokenize-----> Words/Tokens

Libraries used in Tokenizations are Nltk, Spacy


### 2. Stemming

Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or the roots of words known as a lemma . Stemming is important in natural language understanding and Nlp

### 3. Lemmatization
Lemmatization technique is like stemming. The output we will get after lemmatization is called lemma which is root word rather than root stem, the output stem. After lemmitization we will getting a valid word that means the same thing

### 4. Stopwords
Stopwords are the common words in a language that usually donâ€™t carry significant meaning in text analysis.

Examples in English:
`the, is, in, at, of, a, and, to, for, with, on, by, it`

### 5.POS Tagging (Part-of-Speech Tagging)

POS tagging is the process of labeling each word in a sentence with its grammatical role (part of speech) such as noun, verb, adjective, adverb, pronoun, preposition, conjunction, etc.

Example:
Sentence: "The quick brown fox jumps over the lazy dog."
POS tagging result:

The â†’ Determiner (DT)

quick â†’ Adjective (JJ)

brown â†’ Adjective (JJ)

fox â†’ Noun (NN)

jumps â†’ Verb (VBZ)

over â†’ Preposition (IN)

the â†’ Determiner (DT)

lazy â†’ Adjective (JJ)

dog â†’ Noun (NN)

So, POS tagging tells us the syntactic category of each word.

**Why useful?**

Helps in parsing sentences

Improves information retrieval and text classification

Used in machine translation and speech recognition

### 6.Named Entity Recognition (NER)

NER is the process of identifying and classifying real-world entities (like names, locations, organizations, dates, percentages, money, etc.) in text.

Example:
Sentence: "Barack Obama was born in Hawaii on August 4, 1961."
NER result:

Barack Obama â†’ PERSON

Hawaii â†’ LOCATION

August 4, 1961 â†’ DATE

So, NER extracts semantic meaning related to entities.

**Why useful?**

Helps in question answering systems

Used in chatbots and voice assistants

Useful in resume parsing, medical text mining, and business intelligence


### 7.What is One Hot Encoding?

One Hot Encoding is a method to convert **categorical data (labels/text)** into a numerical **binary vector** so that machine learning models can understand it.

* Each category is represented as a vector of `0`s and a single `1`.
* The position of the `1` indicates the category.

---

#### Example

Suppose you have a column **Colors**:

```
Colors = [Red, Green, Blue, Green]
```

Unique categories = `{Red, Green, Blue}`

### One Hot Encoding result:

| Color | Red | Green | Blue |
| ----- | --- | ----- | ---- |
| Red   | 1   | 0     | 0    |
| Green | 0   | 1     | 0    |
| Blue  | 0   | 0     | 1    |
| Green | 0   | 1     | 0    |

---

### Why use it?

* ML models (like Linear Regression, Logistic Regression, Neural Networks) need **numbers** as input, not text.
* One Hot Encoding removes any **ordinal meaning** (e.g., "Red=1, Green=2, Blue=3" would incorrectly imply order).

---

### 8.What is Bag of Words (BoW)?

**Bag of Words** is a simple and commonly used **text representation technique** in Natural Language Processing (NLP).

* It converts text (sentences/documents) into **numerical vectors**.
* It only cares about **word occurrence (frequency or presence)**, not grammar or order.
* The "bag" means we just collect all words, ignoring sequence.

---

### Example

Suppose we have 2 sentences:

1. `"I love NLP"`
2. `"I love Machine Learning"`

### Step 1: Build Vocabulary (all unique words)

```
["I", "love", "NLP", "Machine", "Learning"]
```

### Step 2: Represent each sentence as a vector

| Sentence                  | I | love | NLP | Machine | Learning |
| ------------------------- | - | ---- | --- | ------- | -------- |
| "I love NLP"              | 1 | 1    | 1   | 0       | 0        |
| "I love Machine Learning" | 1 | 1    | 0   | 1       | 1        |

* `1` = word is present
* `0` = word is absent
* (Sometimes we also use **word counts** instead of 0/1)

---

### Variants of BoW

1. **Binary BoW** â†’ Only presence/absence (0 or 1).
2. **Count BoW** â†’ Word frequencies (how many times each word occurs).
3. **TF-IDF (Term Frequency - Inverse Document Frequency)** â†’ A weighted BoW that reduces importance of common words (like *the, is, and*).

---


### Limitations of BoW

* Ignores **word order** (â€œdog bites manâ€ vs â€œman bites dogâ€ look the same).
* Ignores **context/semantics** (meaning).
* High **dimensionality** if vocabulary is large.

---


### 9.What is an N-Gram?

An **N-Gram** is a **sequence of N words (or tokens)** taken from a text.

* **Unigram (1-gram):** single words
* **Bigram (2-gram):** pairs of consecutive words
* **Trigram (3-gram):** three consecutive words
* And so on...

It helps capture some **context and order**, unlike Bag of Words (BoW).

---

### Example

Sentence:

```
"I love Natural Language Processing"
```

* **Unigrams (1-grams):**
  `["I", "love", "Natural", "Language", "Processing"]`

* **Bigrams (2-grams):**
  `["I love", "love Natural", "Natural Language", "Language Processing"]`

* **Trigrams (3-grams):**
  `["I love Natural", "love Natural Language", "Natural Language Processing"]`

---

### Why N-Grams?

* **Unigrams** ignore context (like BoW).
* **Bigrams/Trigrams** add local context by considering word sequences.
* Used in **language models, text classification, sentiment analysis, autocomplete, and speech recognition.**

---

### Limitations

* As **N increases**, vocabulary size grows very fast (sparse representation).
* Still limited in capturing **long-term dependencies**.
* Modern NLP uses **word embeddings + transformers (BERT, GPT)** for richer context.

---


### 10.What is TFâ€“IDF?

**TFâ€“IDF (Term Frequency â€“ Inverse Document Frequency)** is a **weighted version of Bag of Words**.
It measures how important a word is in a **document** relative to the **whole corpus**.

It helps reduce the impact of very common words (like *the, is, of*) and highlight **unique, informative words**.

---

## ğŸ”¹ Formula

1. **Term Frequency (TF):**
   How often a word appears in a document.

   $$
   TF(t,d) = \frac{\text{Count of term t in document d}}{\text{Total terms in document d}}
   $$

2. **Inverse Document Frequency (IDF):**
   How rare the word is across all documents.

   $$
   IDF(t) = \log \left(\frac{\text{Total number of documents}}{\text{Number of documents containing term t}} \right)
   $$

3. **TFâ€“IDF = TF Ã— IDF**

---

Got it ğŸ‘ â€” letâ€™s keep this crisp and clear.

---

### 11.What is **Word Embedding**?

**Word Embedding** is a way of representing words as **dense numerical vectors** in such a way that words with similar meanings are close to each other in vector space.

Unlike Bag of Words or TFâ€“IDF (which are sparse and high-dimensional), embeddings capture **semantic meaning & context**.

Example:

* Vector(â€œkingâ€) â€“ Vector(â€œmanâ€) + Vector(â€œwomanâ€) â‰ˆ Vector(â€œqueenâ€)

---

## ğŸ”¹ Types of Word Embeddings

1. **Frequency-based (count-based)**

   * **Bag of Words (BoW)**
   * **TFâ€“IDF**
   * **Co-occurrence Matrix (Count + SVD)**

2. **Prediction-based (neural embeddings)**

   * **Word2Vec** (CBOW, Skip-gram)
   * **GloVe** (Global Vectors)
   * **FastText** (subword embeddings)

3. **Contextual embeddings (deep learning)**

   * **ELMo** (Embeddings from Language Models)
   * **BERT, GPT, Transformer-based models**

---

## ğŸ”¹ Flow Chart of Word Embeddings

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     Word Embeddings     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                                               â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ Frequency â”‚                                    â”‚ Predictionâ”‚
â”‚  based    â”‚                                    â”‚   based   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚                                               â”‚
 â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                                     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
 â”‚  BoW     â”‚                                     â”‚ Word2Vec â”‚
 â”‚  TFâ€“IDF  â”‚                                     â”‚  GloVe   â”‚
 â”‚  Co-Occ. â”‚                                     â”‚ FastText â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                                                  â”‚ Contextual â”‚
                                                  â”‚ Embeddings â”‚
                                                  â”‚ (ELMo,     â”‚
                                                  â”‚ BERT, GPT) â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---
