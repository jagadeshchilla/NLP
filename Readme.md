## Natural Language Processing

### 1. Tokenization
- It is a process that convert paragraphs/sentences into tokens

paragraphs ----Tokenize-----> Sentences ----Tokenize-----> Words/Tokens

Libraries used in Tokenizations are Nltk, Spacy


### 2. Stemming

Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or the roots of words known as a lemma . Stemming is important in natural language understanding and Nlp

### 3. Lemmatization
Lemmatization technique is like stemming. The output we will get after lemmatization is called lemma which is root word rather than root stem, the output stem. After lemmitization we will getting a valid word that means the same thing